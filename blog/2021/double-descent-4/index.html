<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Deep double descent explained (4/4) | Marc  Lafon</title>
    <meta name="author" content="Marc  Lafon">
    <meta name="description" content="Recent works analyzing over-parameterized regimes.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%A1&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://marclafon.github.io//blog/2021/double-descent-4/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Deep double descent explained (4/4)",
      "description": "Recent works analyzing over-parameterized regimes.",
      "published": "June 17, 2021",
      "authors": [
        {
          "author": "Alexandre Thomas",
          "authorURL": "https://alexandrethm.github.io/",
          "affiliations": [
            {
              "name": "Mines ParisTech & Sorbonne University",
              "url": ""
            }
          ]
        },
        {
          "author": "Marc Lafon",
          "authorURL": "https://marclafon.github.io",
          "affiliations": [
            {
              "name": "Sorbonne University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Marc </span>Lafon</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">menu</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/">about</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/blog/">blog</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/publications/">publications</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Deep double descent explained (4/4)</h1>
        <p>Recent works analyzing over-parameterized regimes.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        \[\newcommand{\Dn}{\mathcal{D}_n}
    \newcommand{\R}{\mathbb{R}}
    \newcommand{\LL}{\mathcal{L}}\]

<h2 id="optimization-in-the-over-parameterized-regime">Optimization in the over-parameterized regime</h2>

<p>For reasons that are still active research, overparameterization
seems beneficial not only in the statistical learning framework, but
from an optimization standpoint as well as it facilitates convergence to
global minima, in particular with the gradient descent procedures.</p>

<p>The optimization problem can be framed as minimizing a certain loss
function \(\LL(w)\) with respect to its parameters \(w \in \mathbb{R}^N\),
such as the square loss \(\LL(w) = \frac{1}{2} \sum_{i=1}^n (f(x_i, w) - y_i)^2\) where
\(\{(x_i, y_i)\}_{i=1}^{n}\) is our given training dataset and
\(f : (\mathbb{R}^d \times \mathbb{R}^N) \rightarrow \mathbb{R}\) is our
model.</p>

<blockquote>
  <p><strong>Exercise 2</strong>.
Assume that \(\ell: \mathcal{Y} \rightarrow \mathbb{R}\) is convex and
\(f : \mathcal{X} \rightarrow \mathcal{Y}\) is linear. Show that
\(\ell \circ f\) is convex.</p>
</blockquote>

<p>When \(f\) is non-linear however (which is habitually the case in deep
learning) the landscape of the loss function is generally non-convex.
Therefore, first order methods such as GD or SGD are likely to converge
and get trapped in spurious local minima, depending on the
initialization. Yet, in the over-parameterized regime where there are
multiple global minima interpolating almost perfectly the data, it seems
that SGD has no problem converging to these solutions, despite the
highly non-convex setting. Recent works are trying to explain this
phenomenon.</p>

<p>For instance, Oymak &amp; Soltanolkotabi (2020)<d-cite key="oymak2020toward"></d-cite>
shows that, for one-hidden layer neural
networks that <em>(1)</em> have smooth activation functions, <em>(2)</em> are
over-parameterized, i.e. \(N \geq C n^2\) where C depends on the
distribution of the data and <em>(3)</em> are initialized with i.i.d.
\(\mathcal{N}(0,1)\) entries, then with high probability GD converges
quickly to a global optimum. Similar results also hold for ReLU
activation functions and for SGD.</p>

<p>In Liu et al. (2020)<d-cite key="liu2020toward"></d-cite>, the authors show that sufficiently over-parameterized
systems, including wide neural networks, generally satisfy a condition
that allows gradient descent to converge efficiently, for a broad class
of problems. They use the PL-condition (from Polyak and Lojasiewicz <d-cite key="polyak1963gradient"></d-cite>)
 which does not require convexity but is sufficient for efficient minimization by GD. One key point is that the
loss function \(\LL(w)\) is generally non-convex in the neighborhood of
minimizers. Due to the over-parameterization, the Hessian matrices
\(\nabla^2 \LL(w)\) are positive semi-definite but not positive definite
in these neighborhoods, which is incompatible with convexity for
non-linear sets of solutions. This is in contrast to the
under-parameterized landscape which generally has multiple isolated
local minima with positive definite Hessian matrices. Figure
below illustrates this.</p>

<div class="l-page">
<div class="col-auto">
<img class="img-fluid rounded z-depth-1" src="/assets/img/posts/double-descent/loss_landscape.png">
</div>
</div>
<div class="caption">
  <p><em>Left :</em> Loss landscape of under-parameterized models, locally convex
at local minima. <em>Right :</em> Loss landscape of over-parameterized models,
incompatible with local convexity. Taken from
Liu et al. (2020)<d-cite key="liu2020toward"></d-cite></p>
</div>

<p>In addition to better convergence guarantees, over-parameterization can
even accelerate optimization. By working with <em>linear</em> neural networks
(hence fixed expressiveness), Arora et al. (2018)<d-cite key="arora2018optimization"></d-cite> 
finds that increasing depth
has an implicit effect on gradient descent, combining certain forms of
<em>momentum</em> and <em>adaptive learning rates</em> (two well-known tools in the
field of optimization). They observe the acceleration for non-linear
networks as well (replacing weight matrices by a product of matrices,
for fixed expressiveness), and even when using explicit acceleration
methods such as Adam.</p>

<h2 id="neural-networks-as-a-physical-system--the-jamming-transition">Neural networks as a physical system : the jamming transition</h2>

<p>In order to study the loss landscape, Spigler et al. (2019)<d-cite key="spigler_jamming_2019"></d-cite> make an
analogy between neural networks and complex physical systems with
non-convex energy landscape, called glassy systems. Indeed, the loss
function can be interpreted as the potential energy of the system \(f\),
with a large number of parameters \(N\) (degrees of freedom). By
considering the hinge loss, the minimization of \(\LL(w;\Dn)\) actually
amounts to a constraint-classification problem (with \(n\) constraints,
\(N\) continuous degrees of freedom), already studied in physics.</p>

<p>Using this analogy, they show that the behavior of deep networks near
the interpolation point is similar to the behavior of some granular
systems, that undergo a critical <em>jamming transition</em> when their density
increases such that they are forced to be in contact one another. In the
under-parameterized regime, not all the training examples can be
classified correctly, which leads to unsatisfied constraints. But in the
over-parameterized regime, there is no stable local minima : the network
reaches a global minima zero training loss.</p>

<p>As illustrated in figure below, the authors are able to quantify
the location of the jamming transition in the \((n, N)\) plane
(considering \(N\) as the <em>effective</em> number of parameters of the
network). Considering a fully-connected network with arbitrary depth,
ReLU activation functions, and a dataset of size \(n\), they give a linear
upper bound on the critical number of parameters \(N^*\) characterizing
the jamming transition : \(N^* \leq \frac{1}{C_0} n\) where \(C_0\) is a
constant. In their experiments, it seems that the bound is tight for
random data but that \(N^*\) increases sub-linearly with \(n\) for
structured data (e.g. MNIST).</p>

<div class="l-body">
<div class="col-auto">
<img class="img-fluid rounded z-depth-1" src="/assets/img/posts/double-descent/jamming-transition.png">
</div>
</div>
<div class="caption">
  <p>\(N\) : degrees of freedom, \(n\) : training examples. Inspired from
Spigler et al. <d-cite key="spigler2018jamming"></d-cite></p>
</div>

<p>Similarly to other works, they observe a peak in test error at the
jamming transition. In Geiger et al. (2020)<d-cite key="geiger2020scaling"></d-cite>,
using the same setting of fixed-depth fully-connected networks, they argue that this may be due to
\(||f||\) diverging near the interpolation point \(N=N^*\). Interestingly,
they also observe that near-optimal generalization can be obtained using
an ensemble average of networks with \(N\) slightly beyond \(N^*\).</p>

<h2 id="conclusion">Conclusion</h2>

<p>From a statistical learning point of view, deep learning is a
challenging setting to study, and some recent empirical successes are not
yet well understood. The double descent phenomenon, arising from
well-chosen inductive biases in the over-parameterized regime, has been
studied in linear settings <d-cite key="Belkin2019"></d-cite> and observed with deep
networks <d-cite key="Nakkiran2019"></d-cite>.</p>

<p>In addition to the references presented in this post, other lines of work seem promising.
Notably, <d-cite key="gissin2019implicit"></d-cite> <d-cite key="neyshabur2015path"></d-cite> <d-cite key="soudry2018implicit"></d-cite> <d-cite key="gunasekar2018implicit"></d-cite>
are working towards a better understanding of the implicit bias induced
by optimization algorithms. Finally, we refer the reader to <a href="http://misha.belkin-wang.org" target="\_blank" rel="external nofollow noopener">subsequent
works of Belkin et al.</a> such as <d-cite key="chen2020multiple"></d-cite>,
that finds <em>multiple descent</em> curves with an arbitrary number of peaks, due to the
interaction between the properties of the data and the inductive biases
of learning algorithms.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2021-05-double-descent.bib"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Marc  Lafon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
